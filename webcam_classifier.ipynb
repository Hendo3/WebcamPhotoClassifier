{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rarfile\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow\n",
        "import cv2\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rarfile\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "tensorflow.__version__\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#put material path here\n",
        "path = \"/content/gdrive/MyDrive/Emotions_detection/Material.rar\"\n",
        "\n",
        "zip_object = rarfile.RarFile(file=path, mode=\"r\")\n",
        "zip_object.extractall(\"./\")\n",
        "zip_object.close\n",
        "\n",
        "VIDEO_HTML = \"\"\"\n",
        "<video autoplay\n",
        " width=%d height=%d style='cursor: pointer;'></video>\n",
        "<script>\n",
        "\n",
        "var video = document.querySelector('video')\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({ video: true })\n",
        "  .then(stream=> video.srcObject = stream)\n",
        "  \n",
        "var data = new Promise(resolve=>{\n",
        "  video.onclick = ()=>{\n",
        "    var canvas = document.createElement('canvas')\n",
        "    var [w,h] = [video.offsetWidth, video.offsetHeight]\n",
        "    canvas.width = w\n",
        "    canvas.height = h\n",
        "    canvas.getContext('2d')\n",
        "          .drawImage(video, 0, 0, w, h)\n",
        "    video.srcObject.getVideoTracks()[0].stop()\n",
        "    video.replaceWith(canvas)\n",
        "    resolve(canvas.toDataURL('image/jpeg', %f))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def take_picture(filename=\"capture_test.jpg\", quality=2, size=(400,300)):\n",
        "\n",
        "    display(HTML(VIDEO_HTML % (size[0],size[1],quality)))\n",
        "    data = eval_js(\"data\")\n",
        "    binary = b64decode(data.split(\",\")[1])\n",
        "    f = io.BytesIO(binary)\n",
        "    return np.asarray(Image.open(f))\n",
        "\n",
        "img_take = take_picture()\n",
        "img_take = cv2.cvtColor(img_take, cv2.COLOR_BGR2RGB)\n",
        "cascade_faces = \"/content/Material/haarcascade_frontalface_default.xml\"\n",
        "model_path = \"/content/Material/modelo_01_expressoes.h5\"\n",
        "face_detection = cv2.CascadeClassifier(cascade_faces)\n",
        "emotional_classifier = load_model(model_path, compile=False)\n",
        "expre = [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
        "original = img_take.copy()\n",
        "faces = face_detection.detectMultiScale(original,scaleFactor=1.1,minNeighbors=3,minSize=(20,20))\n",
        "grey = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "if len(faces) > 0:\n",
        "    for fx, fy, fw, fh in faces:\n",
        "        roi = grey[fy:fy + fh, fx:fx + fw]\n",
        "        roi = cv2.resize(roi, (48,48))\n",
        "        roi = roi.astype(\"float\")/255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "        pred = emotional_classifier.predict(roi)[0]\n",
        "        print(pred)\n",
        "        emotion_prob = np.max(pred)\n",
        "        label = expre[pred.argmax()]\n",
        "        cv2.putText(original, label, (fx, fy - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "        cv2.rectangle(original, (fx, fy), (fx + fw, fy + fh),(0, 0, 255), 2)\n",
        "else:\n",
        "    print(\"No detected face\")\n",
        "\n",
        "cv2_imshow(original)\n",
        "\n",
        "probabilities = np.ones((250, 300, 3), dtype=\"uint8\")*255\n",
        "\n",
        "if len(faces) == 1:\n",
        "    for i, (emotion, proba) in enumerate(zip(expre, pred)):\n",
        "        txt = \"{}: {:2f}%\".format(emotion, proba*100)\n",
        "        w = int(proba*300)\n",
        "        cv2.rectangle(probabilities, (7, (i,*35) + 5), (w, (i*35)+35), (200,250,20), -1)\n",
        "        cv2.putText(probabilities, txt, (10, (i * 35) + 23), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 0), 1, cv2.LINE_AA)\n",
        "    cv2_imshow(probabilities)\n",
        "cv2.imwrite(\"Capture.jpg\", original)\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "Fdyxh4cklzgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "1s6ntn8KmyhX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "webcam_classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}